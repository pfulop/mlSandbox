{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import bcolz\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/run/media/backman/yay/giga-fren/'\n",
    "fname = path + 'giga-fren.release2.fixed'\n",
    "en_fname = fname + '.en/data'\n",
    "fr_fname = fname + '.fr/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_fq = re.compile('^([^?.!]+\\?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ((re_eq.search(eq), re_fq.search(fq))\n",
    "        for eq, fq in zip(open(en_fname), open(fr_fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52331"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [(e.group(), f.group()) for e,f in lines if e and f]\n",
    "len(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is light ?', 'Qu’est-ce que la lumière?'),\n",
       " ('Who are we?', 'Où sommes-nous?'),\n",
       " ('Where did we come from?', \"D'où venons-nous?\"),\n",
       " ('What would we do without it?', 'Que ferions-nous sans elle ?'),\n",
       " ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',\n",
       "  'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?'),\n",
       " ('What is the major aboriginal group on Vancouver Island?',\n",
       "  'Quel est le groupe autochtone principal sur l’île de Vancouver?')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(qs, open(path+'qs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_qs, fr_qs = zip(*qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_qtoks = list(map(simple_toks, fr_qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_qtoks = list(map(simple_toks, en_qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "SOS = 1\n",
    "\n",
    "def toks2ids(sents):\n",
    "    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n",
    "    vocab = sorted(voc_cnt, key = voc_cnt.get, reverse = True)\n",
    "    vocab.insert(PAD, \"<PAD>\")\n",
    "    vocab.insert(SOS, \"<SOS>\")\n",
    "    w2id = {w:i for i,w in enumerate(vocab)}\n",
    "    ids = [[w2id[t] for t in sent] for sent in sents]\n",
    "    return ids, vocab, w2id, voc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_ids, fr_vocab, fr_w2id, fr_counts = toks2ids(fr_qtoks)\n",
    "en_ids, en_vocab, en_w2id, en_counts = toks2ids(en_qtoks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_loc = '/run/media/backman/yay/glove/6B.100d'\n",
    "en_vecs, en_wv_word, en_wv_idx = bcolz.open(glove_loc+'.dat')[:], pickle.load(open(glove_loc+'_words.pkl','rb'), encoding='latin1'), pickle.load(open(glove_loc+'_idx.pkl','rb'), encoding='latin1')\n",
    "en_w2v = {w: en_vecs[en_wv_idx[w]] for w in en_wv_word}\n",
    "n_en_vec, dim_en_vec = en_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = '/run/media/backman/yay/frWac_non_lem_no_postag_no_phrase_200_cbow_cut0.bin'\n",
    "fr_model = KeyedVectors.load_word2vec_format(w2v_path, binary=True,unicode_errors='ignore')\n",
    "ft_voc = fr_model.vocab\n",
    "dim_fr_vec = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec))    \n",
    "    found = 0\n",
    "    \n",
    "    for i, word in enumerate(targ_vocab):\n",
    "        try: \n",
    "            emb[i] = w2v[word]\n",
    "            found+=1\n",
    "        except KeyError: emb[i] = np.random.normal(scale=0.6, size=(dim_vec,))\n",
    "    return emb, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embds, found = create_emb(en_w2v, en_vocab, dim_en_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_embds, found = create_emb(fr_model, fr_vocab, dim_fr_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "en_padded = pad_sequences(en_ids, maxlen, 'int64', 'post', 'post')\n",
    "fr_padded = pad_sequences(fr_ids, maxlen, 'int64', 'post', 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_train, fr_test, en_train, en_test = model_selection.train_test_split(\n",
    "    fr_padded, en_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size = 16):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Var(*sz): return Parameter(Arr(*sz)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arr(*sz): return torch.randn(sz)/math.sqrt(sz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_t(arr): return Variable(torch.LongTensor(arr)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inp, encoder):\n",
    "    batch_size, input_length = inp.size()\n",
    "    hidden = encoder.initHidden(batch_size).cuda()\n",
    "    enc_outputs, hidden = encoder(inp, hidden)\n",
    "    return long_t([SOS]*batch_size), enc_outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2, p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs)\n",
    "        self.W1 = Var(hidden_size, hidden_size)\n",
    "        self.W2 = Var(hidden_size, hidden_size)\n",
    "        self.W3 = Var(emb_size+hidden_size, hidden_size)\n",
    "        self.b2 = Var(hidden_size)\n",
    "        self.b3 = Var(hidden_size)\n",
    "        self.V = Var(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inp, hidden, enc_outputs):\n",
    "        emb_inp = self.emb(inp)\n",
    "        print(enc_outputs, self.W1)\n",
    "        w1e = enc_outputs.mm(self.W1)\n",
    "        w2h = torch.mm(hidden[-1], self.W2) + self.b2\n",
    "        u = F.tanh(w1e + w2h)\n",
    "        a = self.V*u\n",
    "        a = F.softmax(a)\n",
    "        Xa = a * enc_outputs\n",
    "        res =  torch.mm(torch.cat([emb_inp, Xa.squeeze(1)], 1),self.W3)\n",
    "        res = res + self.b3\n",
    "        res, hidden = self.gru(res, hidden)\n",
    "        res = F.log_softmax(self.out(res))\n",
    "        return res, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, targ, encoder, decoder, enc_opt, dec_opt, crit):\n",
    "    decoder_input, encoder_outputs, hidden = encode(inp, encoder)\n",
    "    target_length = targ.size()[1]\n",
    "    \n",
    "    enc_opt.zero_grad(); dec_opt.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, hidden = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        decoder_input = targ[:, di]\n",
    "        loss += crit(decoder_output, decoder_input)\n",
    "        \n",
    "    loss.backward()\n",
    "    enc_opt.step(); dec_opt.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_grad_params(o):\n",
    "    return (p for p in o.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochs(encoder, decoder, n_epochs, print_every = 1000, lr=0.01):\n",
    "    loss_total = 0\n",
    "    \n",
    "    enc_opt = optim.RMSprop(req_grad_params(encoder), lr = lr)\n",
    "    dec_opt = optim.RMSprop(decoder.parameters(), lr = lr)\n",
    "    crit = nn.NLLLoss().cuda()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        fra, eng = get_batch(fr_train, en_train, 64)\n",
    "        inp = long_t(fra)\n",
    "        targ = long_t(eng)\n",
    "        loss = train(inp, targ, encoder, decoder, enc_opt, dec_opt, crit)\n",
    "        loss_total += loss\n",
    "        \n",
    "        if epoch % print_every == print_every -1:\n",
    "            print('%d %d%% %.4f' % (epoch, epoch / n_epochs * 100, loss_total / print_every ))\n",
    "            loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(emb_mat, non_trainable=False):\n",
    "    output_size, emb_size = emb_mat.size()\n",
    "    emb = nn.Embedding(output_size, emb_size)\n",
    "    emb.load_state_dict({'weight': emb_mat})\n",
    "    if non_trainable:\n",
    "        for param in emb.parameters(): \n",
    "            param.requires_grad = False\n",
    "    return emb, emb_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs, True)\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "#                          ,bidirectional=True)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        return self.gru(self.emb(input), hidden)\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_emb_t = torch.FloatTensor(fr_embds).cuda()\n",
    "en_emb_t = torch.FloatTensor(en_embds).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "encoder = EncoderRNN(fr_emb_t, hidden_size).cuda()\n",
    "decoder = AttnDecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30])\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  7.4943e-02  5.9511e-03 -1.8843e-02  ...  -1.2795e-01  2.1465e-02 -1.3614e-01\n",
      "  7.9483e-02  1.9261e-02 -9.8951e-04  ...  -1.9761e-01 -3.7201e-02 -1.6837e-01\n",
      "  6.1664e-02  1.1682e-02 -2.7435e-02  ...  -2.1335e-01 -6.1732e-02 -1.6017e-01\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.3795e-01  3.7539e-01 -7.1583e-02  ...   3.2384e-01 -4.8245e-02  8.7146e-02\n",
      " -1.3793e-01  3.7549e-01 -7.1432e-02  ...   3.2409e-01 -4.8090e-02  8.7253e-02\n",
      " -1.3792e-01  3.7557e-01 -7.1315e-02  ...   3.2428e-01 -4.7970e-02  8.7336e-02\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  9.0921e-02  3.8432e-02 -3.9765e-02  ...  -1.7453e-01 -1.4940e-02  3.3795e-02\n",
      "  2.2165e-01  3.1974e-02 -5.4888e-02  ...  -3.5640e-01 -3.4397e-02  6.7250e-02\n",
      "  2.8385e-01  1.1232e-01 -8.4328e-02  ...  -3.7936e-01 -7.5288e-02  4.8425e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.3779e-01  3.7549e-01 -7.1767e-02  ...   3.2390e-01 -4.8051e-02  8.7029e-02\n",
      " -1.3784e-01  3.7557e-01 -7.1573e-02  ...   3.2416e-01 -4.7936e-02  8.7166e-02\n",
      " -1.3787e-01  3.7563e-01 -7.1422e-02  ...   3.2435e-01 -4.7849e-02  8.7271e-02\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  1.2526e-02  2.8606e-01 -1.9099e-02  ...  -2.2653e-01  1.4052e-01  1.1723e-01\n",
      " -7.3857e-03  2.9981e-01 -4.7707e-02  ...  -2.4077e-01  2.0715e-01  1.8554e-01\n",
      "  9.5702e-02  4.6076e-02 -8.5066e-02  ...  -4.0415e-01  1.7167e-01  2.4726e-01\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.3585e-01  3.7743e-01 -7.2056e-02  ...   3.1511e-01 -4.6755e-02  8.4855e-02\n",
      " -1.3669e-01  3.7689e-01 -7.2237e-02  ...   3.1802e-01 -4.7175e-02  8.5247e-02\n",
      " -1.3719e-01  3.7650e-01 -7.2230e-02  ...   3.2004e-01 -4.7422e-02  8.5658e-02\n",
      "... \n",
      "\n",
      "(61 ,.,.) = \n",
      "  1.1868e-01 -1.9979e-03 -2.0451e-02  ...  -9.7590e-02 -3.2808e-02 -1.3599e-01\n",
      "  1.8216e-01 -3.7860e-02 -6.3212e-03  ...  -8.0339e-02 -4.6104e-02 -7.2474e-02\n",
      "  1.6743e-01  8.4814e-02  2.8176e-02  ...  -2.0216e-01 -6.0346e-02  2.4543e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.1553e-01  3.7115e-01 -7.2034e-02  ...   3.0513e-01 -5.0145e-02  9.9183e-02\n",
      " -1.2306e-01  3.7292e-01 -7.2569e-02  ...   3.1109e-01 -5.0131e-02  9.3672e-02\n",
      " -1.2820e-01  3.7390e-01 -7.2830e-02  ...   3.1524e-01 -4.9813e-02  9.0485e-02\n",
      "\n",
      "(62 ,.,.) = \n",
      "  1.1868e-01 -1.9979e-03 -2.0451e-02  ...  -9.7590e-02 -3.2808e-02 -1.3599e-01\n",
      "  1.0849e-01  2.8175e-03 -5.2618e-03  ...  -1.5157e-01 -8.2459e-02 -1.6329e-01\n",
      "  7.4340e-02 -7.5015e-03 -3.3065e-02  ...  -1.7240e-01 -8.9805e-02 -1.5165e-01\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.3630e-01  3.7626e-01 -7.2502e-02  ...   3.2080e-01 -4.7863e-02  8.6142e-02\n",
      " -1.3689e-01  3.7605e-01 -7.2248e-02  ...   3.2189e-01 -4.7811e-02  8.6390e-02\n",
      " -1.3726e-01  3.7592e-01 -7.2019e-02  ...   3.2268e-01 -4.7770e-02  8.6619e-02\n",
      "\n",
      "(63 ,.,.) = \n",
      "  9.0921e-02  3.8432e-02 -3.9765e-02  ...  -1.7453e-01 -1.4940e-02  3.3795e-02\n",
      "  9.7545e-02  2.4656e-02 -5.8982e-02  ...  -1.4756e-01 -9.3650e-02 -7.5847e-03\n",
      "  1.6053e-01 -2.7471e-02 -7.2663e-02  ...  -1.0909e-01 -7.3765e-02 -2.3601e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.3784e-01  3.7579e-01 -7.1098e-02  ...   3.2477e-01 -4.7711e-02  8.7432e-02\n",
      " -1.3786e-01  3.7580e-01 -7.1062e-02  ...   3.2481e-01 -4.7681e-02  8.7468e-02\n",
      " -1.3788e-01  3.7581e-01 -7.1031e-02  ...   3.2485e-01 -4.7657e-02  8.7496e-02\n",
      "[torch.cuda.FloatTensor of size 64x30x128 (GPU 0)]\n",
      " Variable containing:\n",
      "-0.0769 -0.0419  0.0499  ...   0.0170 -0.1703  0.0375\n",
      " 0.1513  0.1332 -0.0830  ...  -0.1081 -0.0307  0.2273\n",
      " 0.2269 -0.0126  0.0044  ...  -0.0419 -0.2561  0.0752\n",
      "          ...             ⋱             ...          \n",
      " 0.0803  0.0240  0.0831  ...   0.0464  0.0907  0.0833\n",
      " 0.0932 -0.0973 -0.1770  ...  -0.0547 -0.0408 -0.0209\n",
      " 0.1657 -0.1848  0.0132  ...  -0.1020  0.0030  0.0591\n",
      "[torch.cuda.FloatTensor of size 128x128 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "matrix and matrix expected at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:237",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-58ecf62eb64a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainEpochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-95c9265bc418>\u001b[0m in \u001b[0;36mtrainEpochs\u001b[0;34m(encoder, decoder, n_epochs, print_every, lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-83b1c5e9396e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inp, targ, encoder, decoder, enc_opt, dec_opt, crit)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-98777698a35e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, hidden, enc_outputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0memb_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mw1e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mw2h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1e\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw2h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAddmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: matrix and matrix expected at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:237"
     ]
    }
   ],
   "source": [
    "trainEpochs(encoder, decoder, 10000, print_every=500, lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
