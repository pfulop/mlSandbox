{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset \n",
    "import glob\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 36\n",
    "height = 64\n",
    "emb_w = math.ceil(width / 4)\n",
    "emb_h = math.ceil(height / 4)\n",
    "np.random.seed(1)\n",
    "h= 64\n",
    "n = 64\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/run/media/backman/yay/uiD'\n",
    "MODEL2_PATH = '/run/media/backman/yay/uiG'\n",
    "\n",
    "def save_model():\n",
    "    torch.save(netD.state_dict(), MODEL_PATH)\n",
    "    torch.save(netG.state_dict(), MODEL2_PATH)\n",
    "    \n",
    "def load_model():\n",
    "    netD.load_state_dict(torch.load(MODEL_PATH))\n",
    "    netG.load_state_dict(torch.load(MODEL2_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images():\n",
    "    root_dir = '/run/media/backman/yay/unique_uis/combined/'\n",
    "    new_dir = '/run/media/backman/yay/unique_uis/resized/'\n",
    "    files = glob.glob(root_dir + '*.jpg')\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            im = Image.open(file)\n",
    "            im.thumbnail((width,height), Image.ANTIALIAS)\n",
    "            assert im.size == (width,height)\n",
    "            im.save(new_dir+os.path.basename(file),\"JPEG\")\n",
    "        except IOError:\n",
    "            pass\n",
    "        except AssertionError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleUIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, max = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = glob.glob(root_dir + '*.jpg')\n",
    "        if max:\n",
    "            self.files = np.random.permutation(self.files)[:max]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.files[idx])\n",
    "        image = io.imread(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, tuple)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w), mode='constant')\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GoogleUIDataset('/run/media/backman/yay/unique_uis/resized/',\n",
    "                                                   transform=transforms.Compose([\n",
    "#                                                     Rescale((36, 64)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                                   ]),\n",
    "                  max=10000)\n",
    "guid = torch.utils.data.DataLoader(g,\n",
    "                                   bs, True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "    \n",
    "class Reshape(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4*n, emb_h, emb_w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_sub_block(in_c, out_c, sub = False):\n",
    "    conv_sub_arr = []\n",
    "    conv_sub_arr.append(nn.Conv2d(in_c, out_c, 3, padding=1, bias=False))\n",
    "    conv_sub_arr.append(nn.LeakyReLU())\n",
    "#         conv_sub_arr.append(nn.Conv2d(out_c, out_c, 3, padding=1, bias=False))\n",
    "#         conv_sub_arr.append(nn.ReLU())\n",
    "    if sub:\n",
    "        conv_sub_arr.append(nn.Conv2d(out_c, out_c, 3, stride = 2, padding=1, bias=False))\n",
    "        conv_sub_arr.append(nn.LeakyReLU())\n",
    "        conv_sub_arr.append(torch.nn.Dropout())\n",
    "        \n",
    "    conv_sub_arr.append(nn.BatchNorm2d(out_c))\n",
    "    return conv_sub_arr\n",
    "\n",
    "def deconv_up_block(in_c, out_c = None, up = False):\n",
    "    if out_c is None:\n",
    "        out_c = in_c\n",
    "    deconv_sub_arr = []\n",
    "    if up:\n",
    "        deconv_sub_arr.append(nn.Dropout())\n",
    "\n",
    "    deconv_sub_arr.append(nn.Conv2d(in_c, out_c, 3, padding=1, bias=False))\n",
    "    deconv_sub_arr.append(nn.LeakyReLU())\n",
    "#         deconv_sub_arr.append(nn.Conv2d(out_c, out_c, 3, padding=1, bias=False))\n",
    "#         deconv_sub_arr.append(nn.ReLU())\n",
    "    if up:\n",
    "        deconv_sub_arr.append(nn.ConvTranspose2d(out_c, out_c, 2, 2, bias=False))\n",
    "#         deconv_sub_arr.append(nn.Upsample(scale_factor = 2, mode='nearest'))\n",
    "        deconv_sub_arr.append(nn.LeakyReLU())\n",
    "    return deconv_sub_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "\n",
    "  \n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3, n, 3, padding=1, bias=False))\n",
    "        layers += conv_sub_block(n, n*2, True)\n",
    "        layers += conv_sub_block(n*2, n*4, True)\n",
    "#         layers += self.conv_sub_block(n*3, n*4, True)\n",
    "#         layers += self.conv_sub_block(n*4, n*5, True)\n",
    "#         layers += self.conv_sub_block(n*5, n*6, True)\n",
    "#         layers += self.conv_sub_block(n*6, n*7, True)\n",
    "        layers += conv_sub_block(n*4, n*4)\n",
    "        layers.append(Flatten())\n",
    "        layers.append(nn.Linear(4*emb_h*emb_w*n, h))\n",
    " \n",
    "        layers.append(nn.Linear(h,emb_h*emb_w*n*4))\n",
    "        layers.append(Reshape())\n",
    "#         layers += self.deconv_up_block(n*7, n*6, up = True)\n",
    "#         layers += self.deconv_up_block(n*6, n*5, True)\n",
    "#         layers += self.deconv_up_block(n*5, n*4, True)\n",
    "#         layers += self.deconv_up_block(n*4, n*3, True)\n",
    "        layers += deconv_up_block(n*4, n*2, True)\n",
    "        layers += deconv_up_block(n*2, n*2, True)\n",
    "        layers += deconv_up_block(n*2, n)\n",
    "        layers.append(nn.Conv2d(n, 3, 3, padding=1, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(h,4*emb_h*emb_w*n)) \n",
    "        layers.append(Reshape())\n",
    "#         layers += self.deconv_up_block(n*7, n*6, up = True)\n",
    "#         layers += self.deconv_up_block(n*6, n*5, True)\n",
    "#         layers += self.deconv_up_block(n*5, n*4, True)\n",
    "#         layers += self.deconv_up_block(n*4, n*3, True)\n",
    "        layers += deconv_up_block(n*4, n*2, True)\n",
    "        layers += deconv_up_block(n*2, n*2, True)\n",
    "        layers += deconv_up_block(n*2, n)\n",
    "        layers.append(nn.Conv2d(n, 3, 3, padding=1, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = dtype(bs, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D (\n",
       "  (model): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): LeakyReLU (0.01)\n",
       "    (5): Dropout (p = 0.5)\n",
       "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): LeakyReLU (0.01)\n",
       "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): LeakyReLU (0.01)\n",
       "    (11): Dropout (p = 0.5)\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (14): LeakyReLU (0.01)\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (16): Flatten (\n",
       "    )\n",
       "    (17): Linear (36864 -> 64)\n",
       "    (18): Linear (64 -> 36864)\n",
       "    (19): Reshape (\n",
       "    )\n",
       "    (20): Dropout (p = 0.5)\n",
       "    (21): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (22): LeakyReLU (0.01)\n",
       "    (23): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (24): LeakyReLU (0.01)\n",
       "    (25): Dropout (p = 0.5)\n",
       "    (26): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (27): LeakyReLU (0.01)\n",
       "    (28): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (29): LeakyReLU (0.01)\n",
       "    (30): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (31): LeakyReLU (0.01)\n",
       "    (32): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (33): Tanh ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(netG.parameters(), lr =lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Variable(noise)\n",
    "ms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_image(batch):\n",
    "    var = batch\n",
    "    if use_cuda:\n",
    "        var = var.cuda()\n",
    "    return Variable(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_image():\n",
    "    noise.data.normal_(0, 1)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(net):\n",
    "    example =   net(get_fake_image()).data.cpu()\n",
    "    utils.make_grid(example)\n",
    "    im = example / 2 +0.5\n",
    "    im = im.numpy()\n",
    "    plt.imshow(np.transpose(im[1], (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(net, val): \n",
    "    for p in net.parameters(): p.requires_grad = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs = 1, k_t = 0, lambda_k = 0.001, gamma = 0.5):\n",
    "    n_batches = len(guid)\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        data_iter = iter(guid)\n",
    "        step = 0\n",
    "        n = len(guid)\n",
    "        while step < n:\n",
    "            max_D_train = 5 if n - step >= 5 else 1 \n",
    "#             for _ in range(max_D_train):\n",
    "            image = next(data_iter)\n",
    "            step += 1\n",
    "            #real image\n",
    "            real = get_real_image(image)\n",
    "            netD.zero_grad()\n",
    "            loss_real = torch.mean(torch.abs(netD(real) - real))\n",
    "\n",
    "            #fake image\n",
    "            fake = netG(get_fake_image())\n",
    "            loss_fake = torch.mean(torch.abs(netD(fake) - fake))\n",
    "            loss_discriminator = loss_real - k_t * loss_fake\n",
    "            loss_discriminator.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            make_trainable(netD, False)\n",
    "\n",
    "            netG.zero_grad()\n",
    "            #discriminator should not be trainable here\n",
    "            fake = get_fake_image()\n",
    "            loss_generator = torch.mean(torch.abs(netG(fake) - netD(netG(fake))))\n",
    "            loss_generator.backward()\n",
    "            optimizerG.step()\n",
    "            \n",
    "            update = gamma * loss_real - loss_generator\n",
    "            m = loss_real + update\n",
    "            update = update.data[0]\n",
    "            \n",
    "            k_t += lambda_k * update \n",
    "            k_t = max(min(1, k_t), 0)#bout ingore?\n",
    "            \n",
    "            make_trainable(netD, True)\n",
    "            \n",
    "        m = m.data[0]\n",
    "        ms.append(m)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            print(f'loss_generator: {loss_generator.data[0]};\\\n",
    "                  m: {m};\\\n",
    "                  loss_discriminator:  {loss_discriminator.data[0]}')\n",
    "                \n",
    "    return k_t\n",
    "                \n",
    "            \n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:50<00:00, 110.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_generator: 0.13813576102256775;                  m: 0.21428921818733215;                  loss_discriminator:  0.234949991106987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k_t = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAAD8CAYAAABpe3YUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nz9I4Pkvj+CzNfwGK71lLplwGlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7abef2c4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-430703966a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-fe5ef01dacc3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, k_t, lambda_k, gamma)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#discriminator should not be trainable here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fake_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, dim, keepdim)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/_functions/reduce.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, dim, keepdim)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_t = train(2, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_t = train(5, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr / 2\n",
    "adjust_learning_rate(optimizerD,lr)\n",
    "adjust_learning_rate(optimizerG,lr)\n",
    "k_t = train(5, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_t = train(100, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(100, k_t)\n",
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(100, k_t)\n",
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(100, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(200, k_t)\n",
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(200, k_t)\n",
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(200, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr / 2\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr)\n",
    "k_t = train(200, k_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(netG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
